---
title: "PracticalMachineLearning"
output: html_document
---

#Introduction
The practical machine learning final assessment was an involved effort in processing, munging and finally analyzing data.  While I took a more "nuclear" approach to data cleansing that in many real life scenario's may have diminished the accuracy of my model a great deal.  However in the context of this assignment the heavily cleansed data still produced close to 100% accuracy.  While this model was robust enough for the current problem I would have liked a larger testing set for the model in order to validate it's effectiveness

#The Process
### Initial Load
I began the process by loading the data and the environment to include all packages and data that appeared to be prudent.  I also loaded the pml output function earlier, rather than later. 


```{r , cache=TRUE}
library(caret)
library(AppliedPredictiveModeling)
require(randomForest)

setwd("C:/Users/gerson64/Desktop/Dropbox Sync/Dropbox/Coursera/practicalMachineLearning/project/")
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```


After loading the data I inspected the target variable in order to make sure the dataset was fully populated and so that I had an approximate sense of the proportion of each action I was attempting to predict.  

```{r , cache=TRUE}
summary(train$classe)
```

After a visual inspection of the data I noticed that the user name did not match the user names in the test set making it impossible to match on and that the column x was simply a count of the variable, something which could lead to future overfitting.  Because of this I removed it from both the training and test set in order to keep both data sets synced.

```{r , cache=TRUE}
train<-train[,3:length(train)]
test<-test[,3:length(test)]
```


###Prepping for the model
The first model I was going to implement was a random-forest because it is not only a good technique for variable selection but it is also famous for being relatively predictive.  

Unfortunately Random Forest has some hard limits around the number of factors which it can handle.  To work around this I simply converted all factors to numeric variables.  Since many of the factors in the data set were erroneously cast that way the easiest solution appeared to be to just convert all factors to numeric.  

```{r , cache=TRUE}
defactored <- data.frame(sapply(train[,sapply(train, class) == "factor"], as.numeric))
train[,sapply(train, class) == "factor" & names(train) != "classe"] <- defactored[,1:length(defactored) - 1]
test[,sapply(test, class) == "factor"] <- data.frame(sapply(test[,sapply(test, class) == "factor"], as.numeric))
```
Once again this was done on both the train and test set.  

In order to actually make a prediction with Random Forest you need to erase all nulls.  After experimenting with erasing all nulls on a row by row basis, I discovered a majority of my training samples were now gone.  

```{r }
nrow(na.omit(train))
```

To combat this I decided the simpler solution was to erase all columns which were not fully populated in the test set.  While I lost a large number of columns, the columns that were erased wer sparsely populated and couldn't help me develop the neccesary predictive model anyways.

```{r , cache=TRUE}
naCount <- function(x) sum( is.na( x ) )
train <- train[ , sapply(test,naCount) == 0 ]
train <- train[ , sapply(train,naCount) == 0 ]
```

#Modeling
At this point I finally jumped into the modeling stage.  Per best practice I pulled out a training and test set of my training data.  
```{r , cache=TRUE}
inTrain = createDataPartition(train$classe, p = 3/4)[[1]]
training = train[ inTrain,]
testing = train[-inTrain,]
```

The Open Source R implementation of Random Forest interestingly enough does not include the ability to use the . notation for variable selection.  The below function provides a formula object that works functionally similarly. 

```{r , cache=TRUE}
model = as.formula(paste("classe ~ ", paste( names(train)[names(train) != "classe"]  , collapse = " + " )))  
```

Finaly I began the Random Forest process.  I used standard model parameters since Random Forest doesn't require that much tuning to operate properly.  The only variable I did change was the number of tree's, which I set to double the number of variables.  This was an attempt to reign in what I saw as a high potential for overfitting. 

```{r , cache=TRUE}
trainObj <- randomForest( model  , data=training, importance=TRUE, ntree= 106)
```

Using the below formula I create a very quick and dirty confusion vector.  This Vector shows me that out of 4904 testing observations my model only incorrectly predicted 4.  

```{r , cache=TRUE}
summary( as.factor(paste( pred = predict(trainObj, testing) , actual  =  testing$classe )))
```

Since the model was over 99.9 percent accurate I felt comfortable running it against the test data provided.  Since I had used the available variables in the test to help clean my training data there were no issues deploying my model on the test set of data.

```{r , cache=TRUE}
test[,sapply(test, class) == "factor"] <- data.frame(sapply(test[,sapply(test, class) == "factor"], as.numeric))
test <- test[,names(test) %in% names(train) ]
preds <- predict(trainObj, test)

pml_write_files( as.character(preds) )
```